{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing Image Data for the Africa Biomass Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import urllib.request\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trainset.h5', <http.client.HTTPMessage at 0x7f7c28b777f0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"download images\"\"\"\n",
    "\n",
    "\n",
    "url = \"https://share.phys.ethz.ch/~pf/albecker/abc/09072022_1154_train.h5\"\n",
    "filename = \"trainset.h5\"\n",
    "urllib.request.urlretrieve(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols in dataset: <KeysViewHDF5 ['agbd', 'cloud', 'images', 'lat', 'lon', 'scl']>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"module to load trainset\"\"\"\n",
    "#!wget -q  https://share.phys.ethz.ch/~pf/albecker/abc/09072022_1154_train.h5\n",
    "trainset = h5py.File(\"trainset.h5\", \"r\")\n",
    "\n",
    "# columns\n",
    "data_cols = trainset.keys()\n",
    "print(f\"cols in dataset: {data_cols}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check completeness of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agbd : 25036 values, 0 Nans\n",
      "cloud : 25036 values, 0 Nans\n",
      "images : 25036 values, 0 Nans\n",
      "lat : 25036 values, 0 Nans\n",
      "lon : 25036 values, 0 Nans\n",
      "scl : 25036 values, 0 Nans\n"
     ]
    }
   ],
   "source": [
    "for col in data_cols:\n",
    "    tmp = np.array(trainset[col])\n",
    "    # Check for Nans\n",
    "    mask = np.isnan(tmp) | np.equal(tmp, None)\n",
    "    num_na = np.count_nonzero(mask)\n",
    "    print(f\"{col} : {len(tmp)} values, {num_na} Nans\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">âœ… complete data\n",
    "\n",
    "> agbd stands for Above Ground Biomass Density"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import transform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# shape (25036, 15, 15, 12)\n",
    "images_data = np.array(trainset[\"images\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size up images to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the images to !224 by 224  64 for now\n",
    "image_data_resized = np.zeros((len(images_data), 64, 64, 12))\n",
    "for i in range(len(images_data)):\n",
    "    for j in range(12):\n",
    "        image_data_resized[i, :, :, j] = transform.resize(\n",
    "            images_data[i, :, :, j], (64, 64)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature from the VI indices\n",
    "\n",
    "- **NDVI (Normalized Difference Vegetation Index)**: NDVI is widely used for vegetation monitoring and biomass estimation. It is sensitive to the amount and vigor of green vegetation, and has been shown to be strongly correlated with biomass in many studies.\n",
    "\n",
    "- **EVI (Enhanced Vegetation Index)**: EVI is a modified version of NDVI that aims to minimize the effects of atmospheric aerosols and canopy background reflectance. It has been shown to be more effective than NDVI in areas with high biomass, and can be particularly useful for detecting changes in biomass over time.\n",
    "\n",
    "Choice: EVI\n",
    "2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))\n",
    "\n",
    "where NIR, Red, and Blue are the Near Infrared, Red, and Blue bands, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evi_calc(nir, red, blue):\n",
    "    \"\"\"takes in nir, red, blue values and returns image evi\"\"\"\n",
    "    evi_val = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1))\n",
    "    return evi_val\n",
    "\n",
    "\n",
    "# Extract necessary bands\n",
    "nir_band = image_data_resized[:, :, :, 7]  # 8th band\n",
    "red_band = image_data_resized[:, :, :, 3]  # 4th band\n",
    "blue_band = image_data_resized[:, :, :, 1]  # 2nd band\n",
    "\n",
    "# Compute EVI for each image\n",
    "evi_data = evi_calc(nir_band, red_band, blue_band)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_data = np.array(trainset[\"cloud\"])\n",
    "lat_data = np.array(trainset[\"lat\"])\n",
    "lon_data = np.array(trainset[\"lon\"])\n",
    "scl_data = np.array(trainset[\"scl\"])\n",
    "\n",
    "# Resize the cloud\n",
    "cloud_data_resized = np.zeros((len(cloud_data), 64, 64, 1))\n",
    "for i in range(len(cloud_data)):\n",
    "    for j in range(1):\n",
    "        cloud_data_resized[i, :, :, j] = transform.resize(\n",
    "            cloud_data[i, :, :, j], (64, 64)\n",
    "        )\n",
    "# Resize the lat\n",
    "lat_data_resized = np.zeros((len(lat_data), 64, 64, 1))\n",
    "for i in range(len(lat_data)):\n",
    "    for j in range(1):\n",
    "        lat_data_resized[i, :, :, j] = transform.resize(\n",
    "            lat_data[i, :, :, j], (64, 64)\n",
    "        )\n",
    "# Resize the lon\n",
    "lon_data_resized = np.zeros((len(lon_data), 64, 64, 1))\n",
    "for i in range(len(lon_data)):\n",
    "    for j in range(1):\n",
    "        lon_data_resized[i, :, :, j] = transform.resize(\n",
    "            lon_data[i, :, :, j], (64, 64)\n",
    "        )\n",
    "# Resize the scl\n",
    "scl_data_resized = np.zeros((len(scl_data), 64, 64, 1))\n",
    "for i in range(len(images_data)):\n",
    "    for j in range(1):\n",
    "        scl_data_resized[i, :, :, j] = transform.resize(\n",
    "            scl_data[i, :, :, j], (64, 64)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "image_data_normalized = scaler.fit_transform(image_data_resized.reshape(-1, 12))\n",
    "image_data_normalized = image_data_normalized.reshape((len(images_data), 64, 64, 12))\n",
    "\n",
    "cloud_data_normalized = scaler.fit_transform(cloud_data_resized.reshape(-1, 1))\n",
    "cloud_data_normalized = cloud_data_normalized.reshape((len(cloud_data), 64, 64, 1))\n",
    "\n",
    "lat_data_normalized = scaler.fit_transform(lat_data_resized.reshape(-1, 1))\n",
    "lat_data_normalized = lat_data_normalized.reshape((len(lat_data), 64, 64, 1))\n",
    "\n",
    "lon_data_normalized = scaler.fit_transform(lon_data_resized.reshape(-1, 1))\n",
    "lon_data_normalized = lon_data_normalized.reshape((len(lon_data), 64, 64, 1))\n",
    "\n",
    "scl_data_normalized = scaler.fit_transform(scl_data_resized.reshape(-1, 1))\n",
    "scl_data_normalized = scl_data_normalized.reshape((len(scl_data), 64, 64, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B10 is missing\n",
    "# ðŸ’¡ TODO: Fix band issues : Cirrus -> fixed\n",
    "bands = [\"Coastal_Aerosol\", \"Blue\", \"Green\",\"Red\", \"Vegetation_Red_Edge\", \"Vegetation_Red_Edge_2\", \"Vegetation_Red_Edge_3\", \"NIR\", \"Narrow_NIR\", \"Water_Vapor\", \"SWIR_1\", \"SWIR_2\"]\n",
    "\n",
    "for i in range(12):\n",
    "    locals()[bands[i]] = image_data_normalized[:, :, :, i]\n",
    "    #print(f\"{bands[i]} = {locals()[bands[i]].shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring all the data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = np.reshape(cloud_data_normalized,(len(cloud_data_normalized,), -1))\n",
    "agbd = np.array(trainset[\"agbd\"])\n",
    "latitude = np.reshape(lat_data_normalized,(len(lat_data_normalized,), -1))\n",
    "longitude = np.reshape(lon_data_normalized,(len(lon_data_normalized,), -1))\n",
    "scl_data = np.reshape(scl_data_normalized,(len(scl_data_normalized,), -1))\n",
    "ev_index = evi_data.reshape(25036,-1)\n",
    "Coastal_Aerosol_1 = Coastal_Aerosol.reshape(25036,-1)\n",
    "Blue_2 = Blue.reshape(25036,-1)\n",
    "Green_3 = Green.reshape(25036,-1)\n",
    "Red_4 = Red.reshape(25036,-1)\n",
    "Vegetation_Red_Edge_5 = Vegetation_Red_Edge.reshape(25036,-1)\n",
    "Vegetation_Red_Edge_2_6 = Vegetation_Red_Edge_2.reshape(25036,-1)\n",
    "Vegetation_Red_Edge_3_7= Vegetation_Red_Edge_3.reshape(25036,-1)\n",
    "NIR_8 = NIR.reshape(25036,-1)\n",
    "Narrow_NIR_8A = Narrow_NIR.reshape(25036,-1)\n",
    "Water_Vapor_9 = Water_Vapor.reshape(25036,-1)\n",
    "SWIR_1_11 = SWIR_1.reshape(25036,-1)\n",
    "SWIR_2_12 = SWIR_2.reshape(25036,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    'cloud': cloud.flatten(),\n",
    "    'latitude': latitude.flatten(),\n",
    "    'longitude': longitude.flatten(),\n",
    "    'scl': scl_data.flatten(),\n",
    "    'ev_index': ev_index.flatten(),\n",
    "    'Coastal_Aerosol_1': Coastal_Aerosol_1.flatten(),\n",
    "    'Blue_2': Blue_2.flatten(),\n",
    "    'Green_3': Green_3.flatten(),\n",
    "    'Red_4': Red_4.flatten(),\n",
    "    'Vegetation_Red_Edge_5': Vegetation_Red_Edge_5.flatten(),\n",
    "    'Vegetation_Red_Edge_2_6': Vegetation_Red_Edge_2_6.flatten(),\n",
    "    'Vegetation_Red_Edge_3_7': Vegetation_Red_Edge_3_7.flatten(),\n",
    "    'NIR_8': NIR_8.flatten(),\n",
    "    'Narrow_NIR_8A': Narrow_NIR_8A.flatten(),\n",
    "    'Water_Vapor_9': Water_Vapor_9.flatten(),\n",
    "    'SWIR_1_11': SWIR_1_11.flatten(),\n",
    "    'SWIR_2_12': SWIR_2_12.flatten()\n",
    "}\n",
    "\n",
    "# Create X_train\n",
    "X_train = pd.DataFrame(data)\n",
    "\n",
    "y_train = agbd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"flatdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = {\"agbd\" : agbd}\n",
    "y_train = pd.DataFrame(targets)\n",
    "\n",
    "y_train.to_csv(\"targets.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4de7276074beb09e94c04d5e134c141d0f336e3226759f4865dc0d0ce3d2e27e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
